{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Hamdan Abdollah - 2021186"
      ],
      "metadata": {
        "id": "mGF8n9b0oJY0"
      },
      "id": "mGF8n9b0oJY0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e103f17e",
      "metadata": {
        "id": "e103f17e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from nltk.stem import PorterStemmer\n",
        "import tkinter as tk\n",
        "from tkinter import ttk, messagebox, scrolledtext"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b622a7c7",
      "metadata": {
        "id": "b622a7c7"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78932cc6",
      "metadata": {
        "id": "78932cc6"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77db8c88",
      "metadata": {
        "id": "77db8c88"
      },
      "outputs": [],
      "source": [
        "def preprocess(text, stopwords):\n",
        "    text = text.lower() #lowering case\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text) # tokenize using regex\n",
        "    tokens = [stemmer.stem(token) for token in tokens if token not in stopwords] # remove stopwords and apply stemming\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1cac79b",
      "metadata": {
        "id": "e1cac79b"
      },
      "outputs": [],
      "source": [
        "# making the inverted index\n",
        "def build_inverted_index(abstracts, stopwords):\n",
        "    inverted_index = defaultdict(list)\n",
        "    for doc_id, abstract in abstracts.items():\n",
        "        tokens = preprocess(abstract, stopwords)\n",
        "        for position, token in enumerate(tokens):\n",
        "            inverted_index[token].append((doc_id, position))\n",
        "    return inverted_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9ca6d8f",
      "metadata": {
        "id": "a9ca6d8f"
      },
      "outputs": [],
      "source": [
        "# making the positional index\n",
        "def build_positional_index(inverted_index):\n",
        "    positional_index = defaultdict(dict)\n",
        "    for term, postings in inverted_index.items():\n",
        "        for doc_id, position in postings:\n",
        "            if doc_id not in positional_index[term]:\n",
        "                positional_index[term][doc_id] = []\n",
        "            positional_index[term][doc_id].append(position)\n",
        "    return positional_index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba9fe143",
      "metadata": {
        "id": "ba9fe143"
      },
      "source": [
        "# Saving index and loading from device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e72c5c1",
      "metadata": {
        "id": "6e72c5c1"
      },
      "outputs": [],
      "source": [
        "def save_indexes(inverted_index, positional_index, filename=\"indexes.pkl\"):\n",
        "    with open(filename, \"wb\") as file:\n",
        "        pickle.dump({\"inverted_index\": inverted_index, \"positional_index\": positional_index}, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "329e7fc5",
      "metadata": {
        "id": "329e7fc5"
      },
      "outputs": [],
      "source": [
        "def load_indexes(filename=\"indexes.pkl\"):\n",
        "    with open(filename, \"rb\") as file:\n",
        "        data = pickle.load(file)\n",
        "        return data[\"inverted_index\"], data[\"positional_index\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "809c9fc5",
      "metadata": {
        "id": "809c9fc5"
      },
      "source": [
        "# Handling of Multiple Terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88165539",
      "metadata": {
        "id": "88165539"
      },
      "outputs": [],
      "source": [
        "# function for AND query for multiple terms\n",
        "def process_and_query(terms, positional_index):\n",
        "    if not terms:\n",
        "        return set()\n",
        "    result = set(positional_index.get(terms[0], {}).keys()) #documents containing the first term\n",
        "    # AND with documents containing the remaining terms\n",
        "    for term in terms[1:]:\n",
        "        result.intersection_update(set(positional_index.get(term, {}).keys()))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74b73bc7",
      "metadata": {
        "id": "74b73bc7"
      },
      "outputs": [],
      "source": [
        "# function for OR query for multiple terms\n",
        "def process_or_query(terms, positional_index):\n",
        "    if not terms:\n",
        "        return set()\n",
        "    result = set()\n",
        "    # union all docs containing any of the terms\n",
        "    for term in terms:\n",
        "        result.update(set(positional_index.get(term, {}).keys()))\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "749d19cf",
      "metadata": {
        "id": "749d19cf"
      },
      "outputs": [],
      "source": [
        "# processing NOT Query\n",
        "def process_not_query(term, positional_index, all_doc_ids):\n",
        "    doc_ids = set(positional_index.get(term, {}).keys())\n",
        "    return all_doc_ids - doc_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48eb7998",
      "metadata": {
        "id": "48eb7998"
      },
      "outputs": [],
      "source": [
        "# proximity query\n",
        "def process_proximity_query(term1, term2, k, positional_index):\n",
        "    result = set()\n",
        "    # get documents that contain both terms\n",
        "    common_docs = set(positional_index.get(term1, {}).keys()).intersection(set(positional_index.get(term2, {}).keys()))\n",
        "    for doc_id in common_docs:\n",
        "        positions1 = positional_index[term1][doc_id]\n",
        "        positions2 = positional_index[term2][doc_id]\n",
        "        # check for pairs of positions that satisfy the proximity condition\n",
        "        for pos1 in positions1:\n",
        "            for pos2 in positions2:\n",
        "                if abs(pos1 - pos2) <= k + 1:  # k words apart means distance <= k+1\n",
        "                    result.add(doc_id)\n",
        "                    break\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd89f1de",
      "metadata": {
        "id": "cd89f1de"
      },
      "source": [
        "# Giving most importance to\n",
        "\n",
        "> AND,\n",
        "\n",
        ">  then OR\n",
        "\n",
        "> and then NOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7fc7316",
      "metadata": {
        "id": "c7fc7316"
      },
      "outputs": [],
      "source": [
        "def evaluate_query(query, positional_index, all_doc_ids):\n",
        "    query = query.lower()\n",
        "    # applying stemming to the query\n",
        "    stemmed_query = \" \".join([stemmer.stem(term) for term in query.split()])\n",
        "\n",
        "    # do AND operations first\n",
        "    if \" and \" in stemmed_query:\n",
        "        and_parts = stemmed_query.split(\" and \")\n",
        "        and_results = []\n",
        "        for part in and_parts:\n",
        "            and_results.append(evaluate_query(part.strip(), positional_index, all_doc_ids))\n",
        "        result = set.intersection(*map(set, and_results)) # intersect all results\n",
        "        return result\n",
        "\n",
        "    # do OR operations\n",
        "    elif \" or \" in stemmed_query:\n",
        "        or_parts = stemmed_query.split(\" or \")\n",
        "        or_results = []\n",
        "        for part in or_parts:\n",
        "            or_results.append(evaluate_query(part.strip(), positional_index, all_doc_ids))\n",
        "        result = set.union(*map(set, or_results)) # join all results\n",
        "        return result\n",
        "\n",
        "    # do NOT operations\n",
        "    elif \" not \" in stemmed_query:\n",
        "        terms = stemmed_query.split(\" not \")\n",
        "        return process_not_query(terms[1].strip(), positional_index, all_doc_ids)\n",
        "\n",
        "    # proximity queries\n",
        "    elif \"/\" in stemmed_query:\n",
        "        parts = stemmed_query.split(\"/\")\n",
        "        k = int(parts[1].strip())\n",
        "        terms = parts[0].split()\n",
        "        if len(terms) == 2:\n",
        "            return process_proximity_query(terms[0].strip(), terms[1].strip(), k, positional_index)\n",
        "\n",
        "    # single term query\n",
        "    else:\n",
        "        return set(positional_index.get(stemmed_query, {}).keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2b5cf73",
      "metadata": {
        "id": "a2b5cf73"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import chardet\n",
        "\n",
        "def load_abstracts(directory):\n",
        "    abstracts = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        filepath = os.path.join(directory, filename)\n",
        "\n",
        "        # Detect the file encoding\n",
        "        with open(filepath, 'rb') as file:\n",
        "            raw_data = file.read()\n",
        "            encoding = chardet.detect(raw_data)['encoding']\n",
        "\n",
        "        # Read the file using the detected encoding\n",
        "        try:\n",
        "            with open(filepath, 'r', encoding=encoding) as file:\n",
        "                doc_id = int(filename.split('.')[0])\n",
        "                abstracts[doc_id] = file.read()\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file {filename}: {e}\")\n",
        "\n",
        "    return abstracts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf16e3e",
      "metadata": {
        "id": "4bf16e3e"
      },
      "outputs": [],
      "source": [
        "# Load Stopwords\n",
        "def load_stopwords(stopword_file):\n",
        "    with open(stopword_file, 'r', encoding='utf-8') as file:\n",
        "        stopwords = set(file.read().splitlines())\n",
        "    return stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3146530",
      "metadata": {
        "id": "b3146530"
      },
      "source": [
        "# Running a Simple GUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c208970c",
      "metadata": {
        "id": "c208970c"
      },
      "outputs": [],
      "source": [
        "class BooleanRetrievalGUI:\n",
        "    def __init__(self, positional_index, all_doc_ids):\n",
        "        self.positional_index = positional_index\n",
        "        self.all_doc_ids = all_doc_ids\n",
        "\n",
        "        # main window\n",
        "        self.root = tk.Tk()\n",
        "        self.root.title(\"Boolean Retrieval Model\")\n",
        "        self.root.geometry(\"900x400\")#initial window size\n",
        "        self.root.minsize(600, 400)#minimum window size\n",
        "        self.root.columnconfigure(0, weight=1)\n",
        "        self.root.rowconfigure(1, weight=1)\n",
        "\n",
        "        # style\n",
        "        self.style = ttk.Style()\n",
        "        self.style.configure(\"TLabel\", font=(\"Arial\", 12))\n",
        "        self.style.configure(\"TButton\", font=(\"Arial\", 12))\n",
        "        self.style.configure(\"TEntry\", font=(\"Arial\", 12))\n",
        "\n",
        "        # query input field\n",
        "        self.query_label = ttk.Label(self.root, text=\"Enter your query:\")\n",
        "        self.query_label.grid(row=0, column=0, padx=10, pady=10, sticky=\"w\")\n",
        "        self.query_entry = ttk.Entry(self.root, width=50)\n",
        "        self.query_entry.grid(row=0, column=1, padx=10, pady=10, sticky=\"ew\")\n",
        "        self.query_entry.bind(\"<Return>\", lambda event: self.process_query())  # using enter key to submit query\n",
        "\n",
        "        # buttons\n",
        "        self.button_frame = ttk.Frame(self.root)\n",
        "        self.button_frame.grid(row=0, column=2, padx=10, pady=10, sticky=\"e\")\n",
        "        self.submit_button = ttk.Button(self.button_frame, text=\"Submit\", command=self.process_query)\n",
        "        self.submit_button.pack(side=\"left\", padx=5)\n",
        "        self.clear_button = ttk.Button(self.button_frame, text=\"Clear\", command=self.clear_results)\n",
        "        self.clear_button.pack(side=\"left\", padx=5)\n",
        "\n",
        "        # results display\n",
        "        self.result_label = ttk.Label(self.root, text=\"Result Set:\")\n",
        "        self.result_label.grid(row=1, column=0, padx=10, pady=10, sticky=\"nw\")\n",
        "        self.result_text = scrolledtext.ScrolledText(self.root, wrap=tk.WORD, font=(\"Arial\", 12))\n",
        "        self.result_text.grid(row=1, column=1, columnspan=2, padx=10, pady=10, sticky=\"nsew\")\n",
        "\n",
        "        # status bar\n",
        "        self.status_var = tk.StringVar()\n",
        "        self.status_var.set(\"Ready\")\n",
        "        self.status_bar = ttk.Label(self.root, textvariable=self.status_var, relief=tk.SUNKEN, anchor=tk.W)\n",
        "        self.status_bar.grid(row=2, column=0, columnspan=3, sticky=\"ew\")\n",
        "\n",
        "    def process_query(self):\n",
        "        query = self.query_entry.get()\n",
        "        if not query:\n",
        "            messagebox.showwarning(\"Input Error\", \"Please enter a query.\")\n",
        "            return\n",
        "\n",
        "        self.status_var.set(\"Processing query...\")\n",
        "        self.root.update_idletasks()  # update the status bar immediately\n",
        "\n",
        "        result = evaluate_query(query, self.positional_index, self.all_doc_ids)\n",
        "        self.result_text.delete(1.0, tk.END)\n",
        "        self.result_text.insert(tk.END, f\"Result-Set: {sorted(result)}\")\n",
        "\n",
        "        self.status_var.set(\"Query processed successfully\")\n",
        "\n",
        "    def clear_results(self):\n",
        "        self.query_entry.delete(0, tk.END)\n",
        "        self.result_text.delete(1.0, tk.END)\n",
        "        self.status_var.set(\"Ready\")\n",
        "\n",
        "    def run(self):\n",
        "        self.root.mainloop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b37251a1",
      "metadata": {
        "id": "b37251a1",
        "outputId": "caf6150c-4aec-4b0b-ba3d-a45c4ac4f060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading indexes from disk...\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    abstracts_dir = \"Abstracts\"\n",
        "    stopword_file = \"Stopword-List.txt\"\n",
        "    abstracts = load_abstracts(abstracts_dir)\n",
        "    stopwords = load_stopwords(stopword_file)\n",
        "\n",
        "    # check if indexes already exist on disk\n",
        "    index_file = \"indexes.pkl\"\n",
        "    if os.path.exists(index_file):\n",
        "        print(\"Loading indexes from disk...\")\n",
        "        inverted_index, positional_index = load_indexes(index_file)\n",
        "    else:\n",
        "        print(\"Building indexes...\")\n",
        "        inverted_index = build_inverted_index(abstracts, stopwords)\n",
        "        positional_index = build_positional_index(inverted_index)\n",
        "        save_indexes(inverted_index, positional_index, index_file)\n",
        "\n",
        "    all_doc_ids = set(abstracts.keys())\n",
        "\n",
        "    gui = BooleanRetrievalGUI(positional_index, all_doc_ids)\n",
        "    gui.run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8889c49",
      "metadata": {
        "id": "e8889c49"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}